pippo<-tuneRF(Planets_dataset[Planets_dataset_train,-c(12,1)],Planets_dataset[Planets_dataset_train,12], stepFactor=1, improve=1e-5, ntree=500)
pippo<-tuneRF(Planets_dataset[Planets_dataset_train,-c(12,1)],Planets_dataset[Planets_dataset_train,12], stepFactor=2, improve=1e-5, ntree=500)
pippo<-tuneRF(Planets_dataset[Planets_dataset_train,-c(12,1)],Planets_dataset[Planets_dataset_train,12], stepFactor=2, improve=0.05, ntree=500)
pippo<-tuneRF(Planets_dataset[Planets_dataset_train,-c(12,1)],Planets_dataset[Planets_dataset_train,12], stepFactor=1, improve=0.05, ntree=500)
pippo<-tuneRF(Planets_dataset[Planets_dataset_train,-c(12,1)],Planets_dataset[Planets_dataset_train,12], stepFactor=1.1, improve=0.05, ntree=500)
pippo<-tuneRF(Planets_dataset[Planets_dataset_train,-c(12,1)],Planets_dataset[Planets_dataset_train,12], improve=0.05, ntree=500)
pippo<-tuneRF(Planets_dataset[Planets_dataset_train,-c(12,1)],Planets_dataset[Planets_dataset_train,12], improve=1, ntree=500)
pippo<-tuneRF(Planets_dataset[Planets_dataset_train,-c(12,1)],Planets_dataset[Planets_dataset_train,12], improve=1, ntree=5000)
rfor.planet <-randomForest(as.factor(P_H)~P_P+S_T+P_D+P_PN+P_A+P_D_E+P_F+P_T_E+S_R_E+S_L+P_R+P_M+S_S_T,data=Planets_dataset, subset=Planets_dataset_train,localImp = TRUE,importance=TRUE,proximity=TRUE, mtry=4)
rfor.predict<-data.frame(predict(rfor.planet, Planets_dataset_test, type = "class"))
plot(rfor.planet)
legend("top", colnames(rfor.planet$err.rate), fill=1:ncol(rfor.planet$err.rate))
varImpPlot(rfor.planet)
proximityPlot(rfor.planet)
#print(rfor.planet)
#print(importance(rfor.planet,type=2))
rfor.predict["Test"]<-as.factor(Planets_dataset_test[,12])
colnames(rfor.predict)<-c("Predict","Test")
caret::confusionMatrix(table(rfor.predict))
fourfoldplot(table(rfor.predict), color = c("red","darkgreen"),conf.level = 0, margin = 1, main = "Random Forest")
pred_for<-prediction(as.numeric(rfor.predict$Predict),as.numeric(rfor.predict$Test))
roc_for.perf <- performance(pred_for, measure = "tpr", x.measure = "fpr")
autoplot(roc_for.perf)+theme_bw()
pippo<-tuneRF(Planets_dataset[Planets_dataset_train,-c(12,1)],Planets_dataset[Planets_dataset_train,12], ntree=5000)
pippo<-tuneRF(Planets_dataset[Planets_dataset_train,-c(12,1)],Planets_dataset[Planets_dataset_train,12], ntree=5000)
explain_forest(rfor.planet)
Planets_dataset <- data.frame(read_excel("phl_exoplanet_catalog_FINAL.xlsx"))
set.seed(6)
#########Splitting training vs test set
Planets_dataset_train<- sample(500,350)
Planets_dataset_test<-Planets_dataset[-Planets_dataset_train,]
#########Plotting the correlation chart
#chart.Correlation(Planets_dataset[,2:15], histogram=FALSE)
palette = colorRampPalette(c("green", "blue", "red")) (20)
heatmap(x = cor(Planets_dataset[,2:15]), col = palette, symm = TRUE, margins = c(10, 10),main = 'Planet Features',dist(Planets_dataset[,2:15],method = 'euclidean'))
#########Decision Tree
tune_dec.out=tune(rpart ,P_H~P_P+S_T+P_D+P_PN+P_A+P_D_E+P_F+P_T_E+S_R_E+S_L+P_R+P_M+S_S_T, data= Planets_dataset[Planets_dataset_train,], ranges =list(minsplit=c(seq(1, 30, by = 1))))
print(tune_dec.out)
plot(tune_dec.out,type="contour",swapxy = TRUE,mar = c(2, 1, 1, 2))
tree.planet <- rpart(P_H~P_P+S_T+P_D+P_PN+P_A+P_D_E+P_F+P_T_E+S_R_E+S_L+P_R+P_M+S_S_T,data=Planets_dataset,method="class", subset=Planets_dataset_train,minsplit = 9)
fancyRpartPlot(tree.planet,sub = "Planets Habitability", palettes = "OrRd")
tree.predict<-data.frame(predict(tree.planet, Planets_dataset_test, type = "class"))
rpart.plot(tree.planet,box.palette=c("red", "green"),digits=4,extra=106)
tree.predict["Test"]<-as.factor(Planets_dataset_test[,12])
colnames(tree.predict)<-c("Predict","Test")
caret::confusionMatrix(table(tree.predict))
fourfoldplot(table(tree.predict), color = c("red","darkgreen"),conf.level = 0, margin = 1, main = "Decision Tree")
pred_dec<-prediction(as.numeric(tree.predict$Predict),as.numeric(tree.predict$Test))
roc_dec.perf <- performance(pred_dec, measure = "tpr", x.measure = "fpr")
autoplot(roc_dec.perf)+theme_bw()
#########Random Forest
pippo<-tuneRF(Planets_dataset[Planets_dataset_train,-c(12,1)],Planets_dataset[Planets_dataset_train,12], ntree=5000)
rfor.planet <-randomForest(as.factor(P_H)~P_P+S_T+P_D+P_PN+P_A+P_D_E+P_F+P_T_E+S_R_E+S_L+P_R+P_M+S_S_T,data=Planets_dataset, subset=Planets_dataset_train,localImp = TRUE,importance=TRUE,proximity=TRUE, mtry=4)
rfor.predict<-data.frame(predict(rfor.planet, Planets_dataset_test, type = "class"))
explain_forest(rfor.planet)
plot(rfor.planet)
legend("top", colnames(rfor.planet$err.rate), fill=1:ncol(rfor.planet$err.rate))
varImpPlot(rfor.planet)
proximityPlot(rfor.planet)
#print(rfor.planet)
#print(importance(rfor.planet,type=2))
rfor.predict["Test"]<-as.factor(Planets_dataset_test[,12])
colnames(rfor.predict)<-c("Predict","Test")
caret::confusionMatrix(table(rfor.predict))
fourfoldplot(table(rfor.predict), color = c("red","darkgreen"),conf.level = 0, margin = 1, main = "Random Forest")
pred_for<-prediction(as.numeric(rfor.predict$Predict),as.numeric(rfor.predict$Test))
roc_for.perf <- performance(pred_for, measure = "tpr", x.measure = "fpr")
autoplot(roc_for.perf)+theme_bw()
#########SVM
tune_svm_full.out<-tune(svm ,P_H~P_P+S_T+P_D+P_PN+P_A+P_D_E+P_F+P_T_E+S_R_E+S_L+P_R+P_M+S_S_T,data=Planets_dataset[Planets_dataset_train,], kernel="polynomial", ranges =list(cost=c(seq(0.009, 2, by = 0.005))))
print(tune_svm_full.out)
plot(tune_svm_full.out,type="contour",swapxy = TRUE,mar = c(2, 1, 1, 2))
svm.full <- svm(P_H~P_P+S_T+P_D+P_PN+P_A+P_D_E+P_F+P_T_E+S_R_E+S_L+P_R+P_M+S_S_T, data=Planets_dataset[Planets_dataset_train,],type = 'C-classification', kernel="polynomial",cost=0.5)
plot(svm.full,data=Planets_dataset[Planets_dataset_train,],P_H~S_L, ylim = c(-1, 2)) #projection on P_H vs S_L in, the mistaken one are shown in the decision tree
svm.predict_full<-data.frame(predict(svm.full,Planets_dataset[Planets_dataset_train,],type = "class"))
svm.predict_full["T"]<-as.factor(Planets_dataset[Planets_dataset_train,12])
svm_fin_full<-data.frame(svm.predict_full,stringsAsFactors = TRUE)
colnames(svm_fin_full)<-c("Predict","Test")
caret::confusionMatrix(table(svm_fin_full))
fourfoldplot(table(svm_fin_full), color = c("red","darkgreen"),conf.level = 0, margin = 1, main = "SVM_FULL")
pred_svm_full<-prediction(as.numeric(svm_fin_full$Predict),as.numeric(svm_fin_full$Test))
roc_svm_full.perf <- performance(pred_svm_full, measure = "tpr", x.measure = "fpr")
phi_svm_full<-performance(pred_svm_full, "mi")
phi_svm_full@y.values
autoplot(roc_svm_full.perf)+theme_bw()
#########PCA+SVM
pca.train<-Planets_dataset[Planets_dataset_train,]
pca.test<-Planets_dataset[-Planets_dataset_train,]
pca.planet <- prcomp(pca.train[,2:15], center = TRUE,scale. = TRUE)
pca.planet.test  <-  predict(pca.planet, pca.test[,2:15])
autoplot(pca.planet,data=pca.train[,2:15],col="P_H")
#autoplot(pca.planet.test)
fviz_pca_var(pca.planet,col.var = "contrib",gradient.cols = c("red","orange","blue"),repel = TRUE,col.circle = "black",arrowsize = 1,labelsize = 0.5,jitter = list(what = "both", width = 1, height = 1) ) +theme_bw()+theme(plot.title = element_text(hjust = 0.5))
pca_out<-data.frame(pca.planet[["x"]])
train<-pca_out[1:2]
train["H"]<-pca.train[,12]
#print(svm.planet)
#par(mar = c(5, 5, 5, 5))
#plot(svm.planet,data=train,nlevels = 40)
tune_svm.out=tune(svm ,H~.,data=train, kernel="linear", ranges =list(cost=c(seq(0.009, 1, by = 0.001))))
print(tune_svm.out)
plot(tune_svm.out,type="contour",swapxy = TRUE,mar = c(2, 1, 1, 2))
svm.planet <- ksvm(H~.,data=train,type = 'C-svc', kernel="vanilladot",C=0.44)
plot(svm.planet,data=train)
svm.predict<-data.frame(predict(svm.planet,pca.planet.test[,1:2]))
colnames(svm.predict)[1]<-"H"
svm.predict["T"]<-as.factor(pca.test[,12])
svm_fin<-data.frame(svm.predict,stringsAsFactors = TRUE)
colnames(svm_fin)<-c("Predict","Test")
caret::confusionMatrix(table(svm_fin))
fourfoldplot(table(svm_fin), color = c("red","darkgreen"),conf.level = 0, margin = 1, main = "SVM")
pred_svm<-prediction(as.numeric(svm_fin$Predict),as.numeric(svm_fin$Test))
roc_svm.perf <- performance(pred_svm, measure = "tpr", x.measure = "fpr")
phi_svm<-performance(pred_svm, "mi")
phi_svm@y.values
autoplot(roc_svm.perf)+theme_bw()
#########QDA
qda.planet<- qda(P_H~P_P+S_T+P_D+P_PN+P_A+P_D_E+P_F+P_T_E+S_R_E+S_L+P_R+P_M+S_S_T, data=Planets_dataset, subset=Planets_dataset_train)
#partimat(P_H ~ S_L+P_T_E, data=Planets_dataset[Planets_dataset_train,], method="qda")
#plot(qda.planet,P_H~S_L)
#glm.planet
#summary(glm.planet)
qda.prob<-data.frame(predict(qda.planet,Planets_dataset[-Planets_dataset_train,],type = "response"))
qda.prob<-qda.prob["class"]
qda_fin<-data.frame(qda.prob,stringsAsFactors = TRUE)
qda_fin["Test"]<-as.factor(pca.test[,12])
colnames(qda_fin)<-c("Predict","Test")
caret::confusionMatrix(table(qda_fin))
fourfoldplot(table(qda_fin), color = c("red","darkgreen"),conf.level = 0, margin = 1, main = "QDA")
pred_qda<-prediction(as.numeric(qda_fin$Predict),as.numeric(qda_fin$Test))
roc_qda.perf <- performance(pred_qda, measure = "tpr", x.measure = "fpr")
phi_qda<-performance(pred_qda, "phi")
plot(phi_qda)
autoplot(roc_qda.perf)+theme_bw()
#########LDA
lda.planet<- lda(P_H~P_P+S_T+P_D+P_PN+P_A+P_D_E+P_F+P_T_E+S_R_E+S_L+P_R+P_M+S_S_T, data=Planets_dataset, subset=Planets_dataset_train)
plot(lda.planet)
#glm.planet
#summary(glm.planet)
lda.prob<-data.frame(predict(lda.planet,Planets_dataset[-Planets_dataset_train,],type = "response"))
lda.prob<-lda.prob["class"]
lda_fin<-data.frame(lda.prob,stringsAsFactors = TRUE)
lda_fin["Test"]<-as.factor(pca.test[,12])
colnames(lda_fin)<-c("Predict","Test")
caret::confusionMatrix(table(lda_fin))
fourfoldplot(table(lda_fin), color = c("red","darkgreen"),conf.level = 0, margin = 1, main = "QDA")
pred_lda<-prediction(as.numeric(lda_fin$Predict),as.numeric(lda_fin$Test))
roc_lda.perf <- performance(pred_lda, measure = "tpr", x.measure = "fpr")
phi_lda<-performance(pred_lda, "phi")
plot(phi_lda)
autoplot(roc_lda.perf)+theme_bw()
View(Planets_dataset_test)
tree.planet <- rpart(P_H~P_P+S_T+P_D+P_PN+P_A+P_D_E+P_F+P_T_E+S_R_E+S_L+P_R+P_M+S_S_T,data=Planets_dataset,method="class", subset=Planets_dataset_train,minsplit = 6)
fancyRpartPlot(tree.planet,sub = "Planets Habitability", palettes = "OrRd")
tree.predict<-data.frame(predict(tree.planet, Planets_dataset_test, type = "class"))
rpart.plot(tree.planet,box.palette=c("red", "green"),digits=4,extra=106)
tree.predict["Test"]<-as.factor(Planets_dataset_test[,12])
colnames(tree.predict)<-c("Predict","Test")
caret::confusionMatrix(table(tree.predict))
fourfoldplot(table(tree.predict), color = c("red","darkgreen"),conf.level = 0, margin = 1, main = "Decision Tree")
pred_dec<-prediction(as.numeric(tree.predict$Predict),as.numeric(tree.predict$Test))
roc_dec.perf <- performance(pred_dec, measure = "tpr", x.measure = "fpr")
autoplot(roc_dec.perf)+theme_bw()
source('~/GitHub/Exoplanets_Supervised_Learning/Script.R', echo=TRUE)
tune_dec.out=tune(rpart ,P_H~P_P+S_T+P_D+P_PN+P_A+P_D_E+P_F+P_T_E+S_R_E+S_L+P_R+P_M+S_S_T, data= Planets_dataset[Planets_dataset_train,], ranges =list(minsplit=c(seq(1, 30, by = 1))))
print(tune_dec.out)
plot(tune_dec.out,type="contour",swapxy = TRUE,mar = c(2, 1, 1, 2))
print(tune_dec.out)
plot(tune_dec.out,type="contour",swapxy = TRUE,mar = c(2, 1, 1, 2))
source('~/GitHub/Exoplanets_Supervised_Learning/Script.R', echo=TRUE)
sink()
Planets_dataset <- data.frame(read_excel("phl_exoplanet_catalog_FINAL.xlsx"))
set.seed(9)
#########Splitting training vs test set
Planets_dataset_train<- sample(500,350)
Planets_dataset_test<-Planets_dataset[-Planets_dataset_train,]
#########Plotting the correlation chart
#chart.Correlation(Planets_dataset[,2:15], histogram=FALSE)
palette = colorRampPalette(c("green", "blue", "red")) (20)
heatmap(x = cor(Planets_dataset[,2:15]), col = palette, symm = TRUE, margins = c(10, 10),main = 'Planet Features',dist(Planets_dataset[,2:15],method = 'euclidean'))
#########Decision Tree
tune_dec.out=tune(rpart ,P_H~P_P+S_T+P_D+P_PN+P_A+P_D_E+P_F+P_T_E+S_R_E+S_L+P_R+P_M+S_S_T, data= Planets_dataset[Planets_dataset_train,], ranges =list(minsplit=c(seq(1, 30, by = 1))))
print(tune_dec.out)
plot(tune_dec.out,type="contour",swapxy = TRUE,mar = c(2, 1, 1, 2))
tree.planet <- rpart(P_H~P_P+S_T+P_D+P_PN+P_A+P_D_E+P_F+P_T_E+S_R_E+S_L+P_R+P_M+S_S_T,data=Planets_dataset,method="class", subset=Planets_dataset_train,minsplit = 5)
fancyRpartPlot(tree.planet,sub = "Planets Habitability", palettes = "OrRd")
tree.predict<-data.frame(predict(tree.planet, Planets_dataset_test, type = "class"))
rpart.plot(tree.planet,box.palette=c("red", "green"),digits=4,extra=106)
tree.predict["Test"]<-as.factor(Planets_dataset_test[,12])
colnames(tree.predict)<-c("Predict","Test")
caret::confusionMatrix(table(tree.predict))
fourfoldplot(table(tree.predict), color = c("red","darkgreen"),conf.level = 0, margin = 1, main = "Decision Tree")
pred_dec<-prediction(as.numeric(tree.predict$Predict),as.numeric(tree.predict$Test))
roc_dec.perf <- performance(pred_dec, measure = "tpr", x.measure = "fpr")
autoplot(roc_dec.perf)+theme_bw()
#########Random Forest
pippo<-tuneRF(Planets_dataset[Planets_dataset_train,-c(12,1)],Planets_dataset[Planets_dataset_train,12], ntree=5000)
rfor.planet <-randomForest(as.factor(P_H)~P_P+S_T+P_D+P_PN+P_A+P_D_E+P_F+P_T_E+S_R_E+S_L+P_R+P_M+S_S_T,data=Planets_dataset, subset=Planets_dataset_train,localImp = TRUE,importance=TRUE,proximity=TRUE, mtry=4)
rfor.predict<-data.frame(predict(rfor.planet, Planets_dataset_test, type = "class"))
#explain_forest(rfor.planet)
plot(rfor.planet)
legend("top", colnames(rfor.planet$err.rate), fill=1:ncol(rfor.planet$err.rate))
varImpPlot(rfor.planet)
proximityPlot(rfor.planet)
#print(rfor.planet)
#print(importance(rfor.planet,type=2))
rfor.predict["Test"]<-as.factor(Planets_dataset_test[,12])
colnames(rfor.predict)<-c("Predict","Test")
caret::confusionMatrix(table(rfor.predict))
fourfoldplot(table(rfor.predict), color = c("red","darkgreen"),conf.level = 0, margin = 1, main = "Random Forest")
pred_for<-prediction(as.numeric(rfor.predict$Predict),as.numeric(rfor.predict$Test))
roc_for.perf <- performance(pred_for, measure = "tpr", x.measure = "fpr")
autoplot(roc_for.perf)+theme_bw()
#########SVM
tune_svm_full.out<-tune(svm ,P_H~P_P+S_T+P_D+P_PN+P_A+P_D_E+P_F+P_T_E+S_R_E+S_L+P_R+P_M+S_S_T,data=Planets_dataset[Planets_dataset_train,], kernel="polynomial", ranges =list(cost=c(seq(0.009, 2, by = 0.005))))
print(tune_svm_full.out)
plot(tune_svm_full.out,type="contour",swapxy = TRUE,mar = c(2, 1, 1, 2))
svm.full <- svm(P_H~P_P+S_T+P_D+P_PN+P_A+P_D_E+P_F+P_T_E+S_R_E+S_L+P_R+P_M+S_S_T, data=Planets_dataset[Planets_dataset_train,],type = 'C-classification', kernel="polynomial",cost=0.5)
plot(svm.full,data=Planets_dataset[Planets_dataset_train,],P_H~S_L, ylim = c(-1, 2)) #projection on P_H vs S_L in, the mistaken one are shown in the decision tree
svm.predict_full<-data.frame(predict(svm.full,Planets_dataset[Planets_dataset_train,],type = "class"))
svm.predict_full["T"]<-as.factor(Planets_dataset[Planets_dataset_train,12])
svm_fin_full<-data.frame(svm.predict_full,stringsAsFactors = TRUE)
colnames(svm_fin_full)<-c("Predict","Test")
caret::confusionMatrix(table(svm_fin_full))
fourfoldplot(table(svm_fin_full), color = c("red","darkgreen"),conf.level = 0, margin = 1, main = "SVM_FULL")
pred_svm_full<-prediction(as.numeric(svm_fin_full$Predict),as.numeric(svm_fin_full$Test))
roc_svm_full.perf <- performance(pred_svm_full, measure = "tpr", x.measure = "fpr")
phi_svm_full<-performance(pred_svm_full, "mi")
phi_svm_full@y.values
autoplot(roc_svm_full.perf)+theme_bw()
#########PCA+SVM
pca.train<-Planets_dataset[Planets_dataset_train,]
pca.test<-Planets_dataset[-Planets_dataset_train,]
pca.planet <- prcomp(pca.train[,2:15], center = TRUE,scale. = TRUE)
pca.planet.test  <-  predict(pca.planet, pca.test[,2:15])
autoplot(pca.planet,data=pca.train[,2:15],col="P_H")
#autoplot(pca.planet.test)
fviz_pca_var(pca.planet,col.var = "contrib",gradient.cols = c("red","orange","blue"),repel = TRUE,col.circle = "black",arrowsize = 1,labelsize = 0.5,jitter = list(what = "both", width = 1, height = 1) ) +theme_bw()+theme(plot.title = element_text(hjust = 0.5))
pca_out<-data.frame(pca.planet[["x"]])
train<-pca_out[1:2]
train["H"]<-pca.train[,12]
#print(svm.planet)
#par(mar = c(5, 5, 5, 5))
#plot(svm.planet,data=train,nlevels = 40)
tune_svm.out=tune(svm ,H~.,data=train, kernel="linear", ranges =list(cost=c(seq(0.009, 1, by = 0.001))))
print(tune_svm.out)
plot(tune_svm.out,type="contour",swapxy = TRUE,mar = c(2, 1, 1, 2))
svm.planet <- ksvm(H~.,data=train,type = 'C-svc', kernel="vanilladot",C=0.44)
plot(svm.planet,data=train)
svm.predict<-data.frame(predict(svm.planet,pca.planet.test[,1:2]))
colnames(svm.predict)[1]<-"H"
svm.predict["T"]<-as.factor(pca.test[,12])
svm_fin<-data.frame(svm.predict,stringsAsFactors = TRUE)
colnames(svm_fin)<-c("Predict","Test")
caret::confusionMatrix(table(svm_fin))
fourfoldplot(table(svm_fin), color = c("red","darkgreen"),conf.level = 0, margin = 1, main = "SVM")
pred_svm<-prediction(as.numeric(svm_fin$Predict),as.numeric(svm_fin$Test))
roc_svm.perf <- performance(pred_svm, measure = "tpr", x.measure = "fpr")
phi_svm<-performance(pred_svm, "mi")
phi_svm@y.values
autoplot(roc_svm.perf)+theme_bw()
#########QDA
qda.planet<- qda(P_H~P_P+S_T+P_D+P_PN+P_A+P_D_E+P_F+P_T_E+S_R_E+S_L+P_R+P_M+S_S_T, data=Planets_dataset, subset=Planets_dataset_train)
#partimat(P_H ~ S_L+P_T_E, data=Planets_dataset[Planets_dataset_train,], method="qda")
#plot(qda.planet,P_H~S_L)
#glm.planet
#summary(glm.planet)
qda.prob<-data.frame(predict(qda.planet,Planets_dataset[-Planets_dataset_train,],type = "response"))
qda.prob<-qda.prob["class"]
qda_fin<-data.frame(qda.prob,stringsAsFactors = TRUE)
qda_fin["Test"]<-as.factor(pca.test[,12])
colnames(qda_fin)<-c("Predict","Test")
caret::confusionMatrix(table(qda_fin))
fourfoldplot(table(qda_fin), color = c("red","darkgreen"),conf.level = 0, margin = 1, main = "QDA")
pred_qda<-prediction(as.numeric(qda_fin$Predict),as.numeric(qda_fin$Test))
roc_qda.perf <- performance(pred_qda, measure = "tpr", x.measure = "fpr")
phi_qda<-performance(pred_qda, "phi")
plot(phi_qda)
autoplot(roc_qda.perf)+theme_bw()
#########LDA
lda.planet<- lda(P_H~P_P+S_T+P_D+P_PN+P_A+P_D_E+P_F+P_T_E+S_R_E+S_L+P_R+P_M+S_S_T, data=Planets_dataset, subset=Planets_dataset_train)
plot(lda.planet)
#glm.planet
#summary(glm.planet)
lda.prob<-data.frame(predict(lda.planet,Planets_dataset[-Planets_dataset_train,],type = "response"))
lda.prob<-lda.prob["class"]
lda_fin<-data.frame(lda.prob,stringsAsFactors = TRUE)
lda_fin["Test"]<-as.factor(pca.test[,12])
colnames(lda_fin)<-c("Predict","Test")
caret::confusionMatrix(table(lda_fin))
fourfoldplot(table(lda_fin), color = c("red","darkgreen"),conf.level = 0, margin = 1, main = "QDA")
pred_lda<-prediction(as.numeric(lda_fin$Predict),as.numeric(lda_fin$Test))
roc_lda.perf <- performance(pred_lda, measure = "tpr", x.measure = "fpr")
phi_lda<-performance(pred_lda, "phi")
plot(phi_lda)
autoplot(roc_lda.perf)+theme_bw()
set.seed(10)
#########Splitting training vs test set
Planets_dataset_train<- sample(500,350)
Planets_dataset_test<-Planets_dataset[-Planets_dataset_train,]
tune_dec.out=tune(rpart ,P_H~P_P+S_T+P_D+P_PN+P_A+P_D_E+P_F+P_T_E+S_R_E+S_L+P_R+P_M+S_S_T, data= Planets_dataset[Planets_dataset_train,], ranges =list(minsplit=c(seq(1, 30, by = 1))))
print(tune_dec.out)
plot(tune_dec.out,type="contour",swapxy = TRUE,mar = c(2, 1, 1, 2))
tree.planet <- rpart(P_H~P_P+S_T+P_D+P_PN+P_A+P_D_E+P_F+P_T_E+S_R_E+S_L+P_R+P_M+S_S_T,data=Planets_dataset,method="class", subset=Planets_dataset_train,minsplit = 5)
fancyRpartPlot(tree.planet,sub = "Planets Habitability", palettes = "OrRd")
rpart.plot(tree.planet,box.palette=c("red", "green"),digits=4,extra=106)
tree.predict["Test"]<-as.factor(Planets_dataset_test[,12])
colnames(tree.predict)<-c("Predict","Test")
caret::confusionMatrix(table(tree.predict))
fourfoldplot(table(tree.predict), color = c("red","darkgreen"),conf.level = 0, margin = 1, main = "Decision Tree")
pred_dec<-prediction(as.numeric(tree.predict$Predict),as.numeric(tree.predict$Test))
roc_dec.perf <- performance(pred_dec, measure = "tpr", x.measure = "fpr")
autoplot(roc_dec.perf)+theme_bw()
tune_dec.out=tune(rpart ,P_H~P_P+S_T+P_D+P_PN+P_A+P_D_E+P_F+P_T_E+S_R_E+S_L+P_R+P_M+S_S_T, data= Planets_dataset[Planets_dataset_train,], ranges =list(minsplit=c(seq(1, 30, by = 1))))
print(tune_dec.out)
plot(tune_dec.out,type="contour",swapxy = TRUE,mar = c(2, 1, 1, 2))
tree.planet <- rpart(P_H~P_P+S_T+P_D+P_PN+P_A+P_D_E+P_F+P_T_E+S_R_E+S_L+P_R+P_M+S_S_T,data=Planets_dataset,method="class", subset=Planets_dataset_train,minsplit = 5)
fancyRpartPlot(tree.planet,sub = "Planets Habitability", palettes = "OrRd")
tree.predict<-data.frame(predict(tree.planet, Planets_dataset_test, type = "class"))
rpart.plot(tree.planet,box.palette=c("red", "green"),digits=4,extra=106)
tree.predict["Test"]<-as.factor(Planets_dataset_test[,12])
colnames(tree.predict)<-c("Predict","Test")
caret::confusionMatrix(table(tree.predict))
fourfoldplot(table(tree.predict), color = c("red","darkgreen"),conf.level = 0, margin = 1, main = "Decision Tree")
pred_dec<-prediction(as.numeric(tree.predict$Predict),as.numeric(tree.predict$Test))
roc_dec.perf <- performance(pred_dec, measure = "tpr", x.measure = "fpr")
autoplot(roc_dec.perf)+theme_bw()
set.seed(10)
#########Splitting training vs test set
Planets_dataset_train<- sample(500,350)
Planets_dataset_test<-Planets_dataset[-Planets_dataset_train,]
tune_dec.out=tune(rpart ,P_H~P_P+S_T+P_D+P_PN+P_A+P_D_E+P_F+P_T_E+S_R_E+S_L+P_R+P_M+S_S_T, data= Planets_dataset[Planets_dataset_train,], ranges =list(minsplit=c(seq(1, 30, by = 1))))
print(tune_dec.out)
plot(tune_dec.out,type="contour",swapxy = TRUE,mar = c(2, 1, 1, 2))
tree.planet <- rpart(P_H~P_P+S_T+P_D+P_PN+P_A+P_D_E+P_F+P_T_E+S_R_E+S_L+P_R+P_M+S_S_T,data=Planets_dataset,method="class", subset=Planets_dataset_train,minsplit = 5)
fancyRpartPlot(tree.planet,sub = "Planets Habitability", palettes = "OrRd")
tree.predict<-data.frame(predict(tree.planet, Planets_dataset_test, type = "class"))
rpart.plot(tree.planet,box.palette=c("red", "green"),digits=4,extra=106)
tree.predict["Test"]<-as.factor(Planets_dataset_test[,12])
colnames(tree.predict)<-c("Predict","Test")
caret::confusionMatrix(table(tree.predict))
fourfoldplot(table(tree.predict), color = c("red","darkgreen"),conf.level = 0, margin = 1, main = "Decision Tree")
pred_dec<-prediction(as.numeric(tree.predict$Predict),as.numeric(tree.predict$Test))
roc_dec.perf <- performance(pred_dec, measure = "tpr", x.measure = "fpr")
autoplot(roc_dec.perf)+theme_bw()
Planets_dataset <- data.frame(read_excel("phl_exoplanet_catalog_FINAL.xlsx"))
set.seed(10)
#########Splitting training vs test set
Planets_dataset_train<- sample(500,350)
Planets_dataset_test<-Planets_dataset[-Planets_dataset_train,]
#########Plotting the correlation chart
#chart.Correlation(Planets_dataset[,2:15], histogram=FALSE)
palette = colorRampPalette(c("green", "blue", "red")) (20)
heatmap(x = cor(Planets_dataset[,2:15]), col = palette, symm = TRUE, margins = c(10, 10),main = 'Planet Features',dist(Planets_dataset[,2:15],method = 'euclidean'))
#########Decision Tree
tune_dec.out=tune(rpart ,P_H~P_P+S_T+P_D+P_PN+P_A+P_D_E+P_F+P_T_E+S_R_E+S_L+P_R+P_M+S_S_T, data= Planets_dataset[Planets_dataset_train,], ranges =list(minsplit=c(seq(1, 30, by = 1))))
print(tune_dec.out)
plot(tune_dec.out,type="contour",swapxy = TRUE,mar = c(2, 1, 1, 2))
tree.planet <- rpart(P_H~P_P+S_T+P_D+P_PN+P_A+P_D_E+P_F+P_T_E+S_R_E+S_L+P_R+P_M+S_S_T,data=Planets_dataset,method="class", subset=Planets_dataset_train,minsplit = 5)
fancyRpartPlot(tree.planet,sub = "Planets Habitability", palettes = "OrRd")
tree.predict<-data.frame(predict(tree.planet, Planets_dataset_test, type = "class"))
rpart.plot(tree.planet,box.palette=c("red", "green"),digits=4,extra=106)
tree.predict["Test"]<-as.factor(Planets_dataset_test[,12])
colnames(tree.predict)<-c("Predict","Test")
caret::confusionMatrix(table(tree.predict))
fourfoldplot(table(tree.predict), color = c("red","darkgreen"),conf.level = 0, margin = 1, main = "Decision Tree")
pred_dec<-prediction(as.numeric(tree.predict$Predict),as.numeric(tree.predict$Test))
roc_dec.perf <- performance(pred_dec, measure = "tpr", x.measure = "fpr")
autoplot(roc_dec.perf)+theme_bw()
#########Random Forest
pippo<-tuneRF(Planets_dataset[Planets_dataset_train,-c(12,1)],Planets_dataset[Planets_dataset_train,12], ntree=5000)
rfor.planet <-randomForest(as.factor(P_H)~P_P+S_T+P_D+P_PN+P_A+P_D_E+P_F+P_T_E+S_R_E+S_L+P_R+P_M+S_S_T,data=Planets_dataset, subset=Planets_dataset_train,localImp = TRUE,importance=TRUE,proximity=TRUE, mtry=4)
rfor.predict<-data.frame(predict(rfor.planet, Planets_dataset_test, type = "class"))
#explain_forest(rfor.planet)
plot(rfor.planet)
legend("top", colnames(rfor.planet$err.rate), fill=1:ncol(rfor.planet$err.rate))
varImpPlot(rfor.planet)
proximityPlot(rfor.planet)
#print(rfor.planet)
#print(importance(rfor.planet,type=2))
rfor.predict["Test"]<-as.factor(Planets_dataset_test[,12])
colnames(rfor.predict)<-c("Predict","Test")
caret::confusionMatrix(table(rfor.predict))
fourfoldplot(table(rfor.predict), color = c("red","darkgreen"),conf.level = 0, margin = 1, main = "Random Forest")
pred_for<-prediction(as.numeric(rfor.predict$Predict),as.numeric(rfor.predict$Test))
roc_for.perf <- performance(pred_for, measure = "tpr", x.measure = "fpr")
autoplot(roc_for.perf)+theme_bw()
Planets_dataset <- data.frame(read_excel("phl_exoplanet_catalog_FINAL.xlsx"))
set.seed(10)
#########Splitting training vs test set
Planets_dataset_train<- sample(500,350)
Planets_dataset_test<-Planets_dataset[-Planets_dataset_train,]
#########Plotting the correlation chart
#chart.Correlation(Planets_dataset[,2:15], histogram=FALSE)
palette = colorRampPalette(c("green", "blue", "red")) (20)
heatmap(x = cor(Planets_dataset[,2:15]), col = palette, symm = TRUE, margins = c(10, 10),main = 'Planet Features',dist(Planets_dataset[,2:15],method = 'euclidean'))
#########Decision Tree
tune_dec.out=tune(rpart ,P_H~P_P+S_T+P_D+P_PN+P_A+P_D_E+P_F+P_T_E+S_R_E+S_L+P_R+P_M+S_S_T, data= Planets_dataset[Planets_dataset_train,], ranges =list(minsplit=c(seq(1, 30, by = 1))))
print(tune_dec.out)
plot(tune_dec.out,type="contour",swapxy = TRUE,mar = c(2, 1, 1, 2))
tree.planet <- rpart(P_H~P_P+S_T+P_D+P_PN+P_A+P_D_E+P_F+P_T_E+S_R_E+S_L+P_R+P_M+S_S_T,data=Planets_dataset,method="class", subset=Planets_dataset_train,minsplit = 5)
fancyRpartPlot(tree.planet,sub = "Planets Habitability", palettes = "OrRd")
tree.predict<-data.frame(predict(tree.planet, Planets_dataset_test, type = "class"))
rpart.plot(tree.planet,box.palette=c("red", "green"),digits=4,extra=106)
tree.predict["Test"]<-as.factor(Planets_dataset_test[,12])
colnames(tree.predict)<-c("Predict","Test")
caret::confusionMatrix(table(tree.predict))
fourfoldplot(table(tree.predict), color = c("red","darkgreen"),conf.level = 0, margin = 1, main = "Decision Tree")
pred_dec<-prediction(as.numeric(tree.predict$Predict),as.numeric(tree.predict$Test))
roc_dec.perf <- performance(pred_dec, measure = "tpr", x.measure = "fpr")
autoplot(roc_dec.perf)+theme_bw()
#########Random Forest
pippo<-tuneRF(Planets_dataset[Planets_dataset_train,-c(12,1)],Planets_dataset[Planets_dataset_train,12], ntree=5000)
rfor.planet <-randomForest(as.factor(P_H)~P_P+S_T+P_D+P_PN+P_A+P_D_E+P_F+P_T_E+S_R_E+S_L+P_R+P_M+S_S_T,data=Planets_dataset, subset=Planets_dataset_train,localImp = TRUE,importance=TRUE,proximity=TRUE, mtry=4)
rfor.predict<-data.frame(predict(rfor.planet, Planets_dataset_test, type = "class"))
#explain_forest(rfor.planet)
plot(rfor.planet)
legend("top", colnames(rfor.planet$err.rate), fill=1:ncol(rfor.planet$err.rate))
varImpPlot(rfor.planet)
proximityPlot(rfor.planet)
#print(rfor.planet)
#print(importance(rfor.planet,type=2))
rfor.predict["Test"]<-as.factor(Planets_dataset_test[,12])
colnames(rfor.predict)<-c("Predict","Test")
caret::confusionMatrix(table(rfor.predict))
fourfoldplot(table(rfor.predict), color = c("red","darkgreen"),conf.level = 0, margin = 1, main = "Random Forest")
pred_for<-prediction(as.numeric(rfor.predict$Predict),as.numeric(rfor.predict$Test))
roc_for.perf <- performance(pred_for, measure = "tpr", x.measure = "fpr")
autoplot(roc_for.perf)+theme_bw()
#########SVM
tune_svm_full.out<-tune(svm ,P_H~P_P+S_T+P_D+P_PN+P_A+P_D_E+P_F+P_T_E+S_R_E+S_L+P_R+P_M+S_S_T,data=Planets_dataset[Planets_dataset_train,], kernel="polynomial", ranges =list(cost=c(seq(0.009, 2, by = 0.005))))
print(tune_svm_full.out)
plot(tune_svm_full.out,type="contour",swapxy = TRUE,mar = c(2, 1, 1, 2))
svm.full <- svm(P_H~P_P+S_T+P_D+P_PN+P_A+P_D_E+P_F+P_T_E+S_R_E+S_L+P_R+P_M+S_S_T, data=Planets_dataset[Planets_dataset_train,],type = 'C-classification', kernel="polynomial",cost=0.5)
plot(svm.full,data=Planets_dataset[Planets_dataset_train,],P_H~S_L, ylim = c(-1, 2)) #projection on P_H vs S_L in, the mistaken one are shown in the decision tree
svm.predict_full<-data.frame(predict(svm.full,Planets_dataset[Planets_dataset_train,],type = "class"))
svm.predict_full["T"]<-as.factor(Planets_dataset[Planets_dataset_train,12])
svm_fin_full<-data.frame(svm.predict_full,stringsAsFactors = TRUE)
colnames(svm_fin_full)<-c("Predict","Test")
caret::confusionMatrix(table(svm_fin_full))
fourfoldplot(table(svm_fin_full), color = c("red","darkgreen"),conf.level = 0, margin = 1, main = "SVM_FULL")
pred_svm_full<-prediction(as.numeric(svm_fin_full$Predict),as.numeric(svm_fin_full$Test))
roc_svm_full.perf <- performance(pred_svm_full, measure = "tpr", x.measure = "fpr")
phi_svm_full<-performance(pred_svm_full, "mi")
phi_svm_full@y.values
autoplot(roc_svm_full.perf)+theme_bw()
#########PCA+SVM
pca.train<-Planets_dataset[Planets_dataset_train,]
pca.test<-Planets_dataset[-Planets_dataset_train,]
pca.planet <- prcomp(pca.train[,2:15], center = TRUE,scale. = TRUE)
pca.planet.test  <-  predict(pca.planet, pca.test[,2:15])
autoplot(pca.planet,data=pca.train[,2:15],col="P_H")
#autoplot(pca.planet.test)
fviz_pca_var(pca.planet,col.var = "contrib",gradient.cols = c("red","orange","blue"),repel = TRUE,col.circle = "black",arrowsize = 1,labelsize = 0.5,jitter = list(what = "both", width = 1, height = 1) ) +theme_bw()+theme(plot.title = element_text(hjust = 0.5))
pca_out<-data.frame(pca.planet[["x"]])
train<-pca_out[1:2]
train["H"]<-pca.train[,12]
#print(svm.planet)
#par(mar = c(5, 5, 5, 5))
#plot(svm.planet,data=train,nlevels = 40)
tune_svm.out=tune(svm ,H~.,data=train, kernel="linear", ranges =list(cost=c(seq(0.009, 1, by = 0.001))))
print(tune_svm.out)
plot(tune_svm.out,type="contour",swapxy = TRUE,mar = c(2, 1, 1, 2))
svm.planet <- ksvm(H~.,data=train,type = 'C-svc', kernel="vanilladot",C=0.44)
plot(svm.planet,data=train)
svm.predict<-data.frame(predict(svm.planet,pca.planet.test[,1:2]))
colnames(svm.predict)[1]<-"H"
svm.predict["T"]<-as.factor(pca.test[,12])
svm_fin<-data.frame(svm.predict,stringsAsFactors = TRUE)
colnames(svm_fin)<-c("Predict","Test")
caret::confusionMatrix(table(svm_fin))
fourfoldplot(table(svm_fin), color = c("red","darkgreen"),conf.level = 0, margin = 1, main = "SVM")
pred_svm<-prediction(as.numeric(svm_fin$Predict),as.numeric(svm_fin$Test))
roc_svm.perf <- performance(pred_svm, measure = "tpr", x.measure = "fpr")
phi_svm<-performance(pred_svm, "mi")
phi_svm@y.values
autoplot(roc_svm.perf)+theme_bw()
#########QDA
qda.planet<- qda(P_H~P_P+S_T+P_D+P_PN+P_A+P_D_E+P_F+P_T_E+S_R_E+S_L+P_R+P_M+S_S_T, data=Planets_dataset, subset=Planets_dataset_train)
#partimat(P_H ~ S_L+P_T_E, data=Planets_dataset[Planets_dataset_train,], method="qda")
#plot(qda.planet,P_H~S_L)
#glm.planet
#summary(glm.planet)
qda.prob<-data.frame(predict(qda.planet,Planets_dataset[-Planets_dataset_train,],type = "response"))
qda.prob<-qda.prob["class"]
qda_fin<-data.frame(qda.prob,stringsAsFactors = TRUE)
qda_fin["Test"]<-as.factor(pca.test[,12])
colnames(qda_fin)<-c("Predict","Test")
caret::confusionMatrix(table(qda_fin))
fourfoldplot(table(qda_fin), color = c("red","darkgreen"),conf.level = 0, margin = 1, main = "QDA")
pred_qda<-prediction(as.numeric(qda_fin$Predict),as.numeric(qda_fin$Test))
roc_qda.perf <- performance(pred_qda, measure = "tpr", x.measure = "fpr")
phi_qda<-performance(pred_qda, "phi")
plot(phi_qda)
autoplot(roc_qda.perf)+theme_bw()
#########LDA
lda.planet<- lda(P_H~P_P+S_T+P_D+P_PN+P_A+P_D_E+P_F+P_T_E+S_R_E+S_L+P_R+P_M+S_S_T, data=Planets_dataset, subset=Planets_dataset_train)
plot(lda.planet)
#glm.planet
#summary(glm.planet)
lda.prob<-data.frame(predict(lda.planet,Planets_dataset[-Planets_dataset_train,],type = "response"))
lda.prob<-lda.prob["class"]
lda_fin<-data.frame(lda.prob,stringsAsFactors = TRUE)
lda_fin["Test"]<-as.factor(pca.test[,12])
colnames(lda_fin)<-c("Predict","Test")
caret::confusionMatrix(table(lda_fin))
fourfoldplot(table(lda_fin), color = c("red","darkgreen"),conf.level = 0, margin = 1, main = "QDA")
pred_lda<-prediction(as.numeric(lda_fin$Predict),as.numeric(lda_fin$Test))
roc_lda.perf <- performance(pred_lda, measure = "tpr", x.measure = "fpr")
phi_lda<-performance(pred_lda, "phi")
plot(phi_lda)
autoplot(roc_lda.perf)+theme_bw()
